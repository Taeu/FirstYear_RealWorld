{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Reweighting a probability distribution to a different temperature\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reweight_distribution(original_distribution, temperature = 0.5):\n",
    "    distribution = np.log(original_distribution)/temperature\n",
    "    distribution = np.exp(distribution)\n",
    "    return distribution / np.sum(distribution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "# 8.2 Downloading and parsing the initial text file\n",
    "\n",
    "import keras\n",
    "\n",
    "path = keras.utils.get_file('nietzsche.txt', origin = 'https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:',len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sequences: 200278\n"
     ]
    }
   ],
   "source": [
    "# 8.3 Vectorizing sequences of characters\n",
    "\n",
    "maxlen = 60 # extract sequences of 60 characters.\n",
    "step = 3 # sample a enw sequences every three characters\n",
    "sentences = [] # holds the extracted sequences\n",
    "next_chars = [] # holds the targets\n",
    "\n",
    "for i in range(0, len(text)- maxlen, step) : \n",
    "    sentences.append(text[i:i+maxlen])\n",
    "    next_chars.append(text[i+maxlen])\n",
    "\n",
    "print('# of sequences:',len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: 58\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = dict((char,chars.index(char)) for char in chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vactorization ...\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype = np.bool)\n",
    "y = np.zeros((len(sentences),len(chars)),dtype = np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i,t,char_indices[char]] = 1\n",
    "    y[i,char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulding the Network\n",
    "# 8.4 Single_layer LSTM model for next-character prediction\n",
    "\n",
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen,len(chars))))\n",
    "model.add(layers.Dense(len(chars),activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image('model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.5 Model compilation configuration\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy',optimizer= optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.6 Function to sample the next character given the model's predicitions\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.multinomial.html \n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds)/temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds/np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1,preds,1) \n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 403s 2ms/step - loss: 1.6454\n",
      "---Generating with seed : \"ted and super-imposed, rather\n",
      "than actually built: this is o\"\n",
      "---temperature: 0.2\n",
      "ted and super-imposed, rather\n",
      "than actually built: this is one of the problem the religious an all the problem the experience of the moral of the rearon of the moral the profound of the reality of a person of the comparience of the many and in the reality and the problem and the\n",
      "end a more the many and in the experience of the problem and in the persion of the moral and in the present and the problem the origin and in the problem and in the entration of th---temperature: 0.5\n",
      "lem the origin and in the problem and in the entration of the conceptions of contence the manger and the religious the personcess to the restruble be sour an are all the constinct of the more the convelting and the proves and whole such a problem the great which he higher a perface of the great persier, the great any all the pass an all religion of the long there is the philosophers his vilsude a pathing the dees tention of a man whole restrement of the mo---temperature: 1.0\n",
      "pathing the dees tention of a man whole restrement of the most culture dirlest\n",
      "an impartually instrument,\n",
      "concepsed of himself and\n",
      "excrication. sopention of\n",
      "an\n",
      "stalling \"fait feal wellrity the love--a pession how against had the stabnt the\n",
      "broughsh, and of these anly in feeling the\n",
      "asmediment\n",
      "dispul part as state \"in emil one eruluse indiseder of the never concoural wershouser\n",
      "cholad for perrect result estainged every at a cortual alts,\n",
      "the curiom the \"man---temperature: 1.2\n",
      "esult estainged every at a cortual alts,\n",
      "the curiom the \"manking afternown this it forwhed the pirisimany and conscarationfmang\"\n",
      "findaderpliating\n",
      "high becay scarncie wort,\n",
      "whoer 3fich toor must bly i he world\n",
      "at a fecty which deding,\n",
      "pnoserd and\n",
      "ivilated at gance past for is, as an fawious; expental seatman man callows the eqvezrecess of in c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rude faith of\n",
      "the prove posseme of ever proment,\n",
      "in all fitus chinary\"\n",
      "through formoted-yvald\"\n",
      "diverfuled its are naepoch 2\n",
      "Epoch 1/1\n",
      " 35456/200278 [====>.........................] - ETA: 5:44 - loss: 1.5595"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a42ba7374292>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mstart_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mgenerated_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 8.7 Text-generation loop\n",
    "\n",
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1,60):\n",
    "    print('epoch', epoch)\n",
    "    model.fit(x,y,batch_size =128, epochs =1)\n",
    "    start_index = random.randint(0,len(text)-maxlen-1)\n",
    "    generated_text = text[start_index: start_index+maxlen]\n",
    "    print('---Generating with seed : \"' + generated_text + '\"')\n",
    "    \n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('---temperature:',temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "        \n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1,maxlen,len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0,t,char_indices[char]] = 1.\n",
    "            preds = model.predict(sampled,verbose=0)[0]\n",
    "            next_index = sample(preds,temperature)\n",
    "            next_char = chars[next_index]\n",
    "            \n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "            sys.stdout.write(next_char)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 8.1.5\n",
    "\n",
    "- You can generate discrete sequence data by training a model to predict the next tokens, given previous tokens\n",
    "- Sampling the next token requires balance btw adhering to what the model judges likely, and introducing randomness. (One way to handle this is the notion of softmax temperature.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
