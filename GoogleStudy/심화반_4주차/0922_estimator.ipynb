{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "if sys.version_info < (3,0,0):\n",
    "    from urllib import urlopen\n",
    "else :\n",
    "    from urllib.request import urlopen\n",
    "    \n",
    "tf_version = tf.__version__\n",
    "print(tf_version)\n",
    "\n",
    "assert \"1.1\" <= tf_version, \"x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/tmp/tf_dataset_api\"\n",
    "\n",
    "path_dataset = path +os.sep + \"dataset\"\n",
    "file_train = path_dataset + os.sep + \"iris_training.csv\"\n",
    "file_test = path_dataset + os.sep + \"iris_test.csv\"\n",
    "\n",
    "url_train=\"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "url_test = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "def downloadDataset(url,file):\n",
    "    if not os.path.exists(path_dataset):\n",
    "        os.makedirs(path_dataset)\n",
    "    if not os.path.exists(file):\n",
    "        data = urlopen(url).read()\n",
    "        with open(file, \"wb\") as f:\n",
    "            f.write(data)\n",
    "            f.close()\n",
    "            \n",
    "downloadDataset(url_train,file_train)\n",
    "downloadDataset(url_test,file_test)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CSV features in our training & test data\n",
    "feature_names = [\n",
    "    'SepalLength',#꽃받침 길이\n",
    "    'SepalWidth',#꽃받침 넓이\n",
    "    'PetalLength',#꽃잎 길이\n",
    "    'PetalWidth']#꽃잎 넓이\n",
    "\n",
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_input_fn(file_path, perform_shuffle=False, repeat_count=1):\n",
    "    def decode_csv(line):\n",
    "        parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]])\n",
    "        label = parsed_line[-1:]  # Last element is the label\n",
    "        del parsed_line[-1]  # Delete last element\n",
    "        features = parsed_line  # Everything but last elements are the features\n",
    "        d = dict(zip(feature_names, features)), label\n",
    "        return d\n",
    "\n",
    "    dataset = (tf.data.TextLineDataset(file_path)  # Read text file\n",
    "               .skip(1)  # Skip header row\n",
    "               .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
    "    if perform_shuffle:\n",
    "        # Randomizes input using a window of 256 elements (read into memory)\n",
    "        dataset = dataset.shuffle(buffer_size=256)\n",
    "    dataset = dataset.repeat(repeat_count)  # Repeats dataset this # times\n",
    "    dataset = dataset.batch(32)  # Batch size to use\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-896ea0df8b41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnext_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_input_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Will return 32 random elements\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-56e60ec914e4>\u001b[0m in \u001b[0;36mmy_input_fn\u001b[1;34m(file_path, perform_shuffle, repeat_count)\u001b[0m\n\u001b[0;32m     10\u001b[0m     dataset = (tf.data.TextLineDataset(file_path)  # Read text file\n\u001b[0;32m     11\u001b[0m                \u001b[1;33m.\u001b[0m\u001b[0mskip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Skip header row\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                .map(decode_csv))  # Transform each elem by applying decode_csv fn\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mperform_shuffle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Randomizes input using a window of 256 elements (read into memory)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[0;32m    836\u001b[0m     \"\"\"\n\u001b[0;32m    837\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    839\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mParallelMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_map_func\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1826\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1828\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_as_variant_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\u001b[0m in \u001b[0;36madd_to_graph\u001b[1;34m(self, g)\u001b[0m\n\u001b[0;32m    486\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[1;34m\"\"\"Adds this function into the graph g.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_definition_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[1;31m# Adds this function into 'g'.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;34m\"\"\"Creates the function definition if it's not created yet.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m       \u001b[1;31m# Call func and gather the output tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemp_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m       \u001b[1;31m# There is no way of distinguishing between a function not returning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mtf_map_func\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   1812\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1813\u001b[0m       self._output_shapes = nest.pack_sequence_as(\n\u001b[1;32m-> 1814\u001b[1;33m           ret, [t.get_shape() for t in nest.flatten(ret)])\n\u001b[0m\u001b[0;32m   1815\u001b[0m       self._output_types = nest.pack_sequence_as(\n\u001b[0;32m   1816\u001b[0m           ret, [t.dtype for t in nest.flatten(ret)])\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1812\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1813\u001b[0m       self._output_shapes = nest.pack_sequence_as(\n\u001b[1;32m-> 1814\u001b[1;33m           ret, [t.get_shape() for t in nest.flatten(ret)])\n\u001b[0m\u001b[0;32m   1815\u001b[0m       self._output_types = nest.pack_sequence_as(\n\u001b[0;32m   1816\u001b[0m           ret, [t.dtype for t in nest.flatten(ret)])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'get_shape'"
     ]
    }
   ],
   "source": [
    "next_batch = my_input_fn(file_train, True)  # Will return 32 random elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
