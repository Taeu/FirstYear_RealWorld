{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "if sys.version_info < (3, 0, 0):\n",
    "    from urllib import urlopen\n",
    "else:\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "# Check that we have correct TensorFlow version installed\n",
    "tf_version = tf.__version__\n",
    "print(\"TensorFlow version: {}\".format(tf_version))\n",
    "assert \"1.1\" <= tf_version, \"TensorFlow r1.3 or later is needed\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows users: You only need to change PATH, rest is platform independent\n",
    "PATH = \"/tmp/tf_dataset_and_estimator_apis\"\n",
    "\n",
    "# Fetch and store Training and Test dataset files\n",
    "PATH_DATASET = PATH + os.sep + \"dataset\"\n",
    "FILE_TRAIN = PATH_DATASET + os.sep + \"iris_training.csv\"\n",
    "FILE_TEST = PATH_DATASET + os.sep + \"iris_test.csv\"\n",
    "URL_TRAIN = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "URL_TEST = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "\n",
    "def downloadDataset(url, file):\n",
    "    if not os.path.exists(PATH_DATASET):\n",
    "        os.makedirs(PATH_DATASET)\n",
    "    if not os.path.exists(file):\n",
    "        data = urlopen(url).read()\n",
    "        with open(file, \"wb\") as f:\n",
    "            f.write(data)\n",
    "            f.close()\n",
    "downloadDataset(URL_TRAIN, FILE_TRAIN)\n",
    "downloadDataset(URL_TEST, FILE_TEST)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터세트 표현\n",
    "데이터 세트를 설명하기위해 먼저 특징 목록을 생성 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CSV features in our training & test data\n",
    "feature_names = [\n",
    "    'SepalLength',#꽃받침 길이\n",
    "    'SepalWidth',#꽃받침 넓이\n",
    "    'PetalLength',#꽃잎 길이\n",
    "    'PetalWidth']#꽃잎 넓이\n",
    "\n",
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 반환값\n",
    "모델을 훈련시킬 때는 입력 파일을 읽어와서 특징 및 레이블 데이터를 반환하는 함수가 필요 합니다.\n",
    "반환 값은\n",
    "    첫 번째 요소는 각 입력 특징이 키이고 그다음이 훈련 배치에 대한 값의 목록인 dict여야 합니다.\n",
    "    두 전째 요소는 훈련 배치에 대한 레이블 목록 입니다.\n",
    "\n",
    "여기서는 입력 특징과, 훈련 레이블로 구성된 배치를 반환할 것이므로 이는 반환문에 포함되는 모든 목록의 길이가 같을 것이라는 의미입니다.\n",
    "기술적으로 설명하자면 여기서 목록 이라고 지칭한 것은 실제로는 1차원 텐서를 의미합니다.\n",
    "\n",
    "### input_fn \n",
    "input_fn을 간단히 재사용할 수 있도록 몇 가지 인수를 추가하겠습니다.이를 통해 각각 설정이 다른 입력 함수를 만들 수 있습니다.\n",
    "    file_path:읽어올 데이터 파일\n",
    "    perform_shuffle:레코드 순서를 무작위로 할지 여부입니다.\n",
    "    repeat_count:데이터세트에서 레코드에 대해 반복할 횟수입니다. 예를 들어, 1을 지정하면 각 레코드를 한 번만 읽어옵니다.\n",
    "    None을 지정하면 영구적으로 계속 반복됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_input_fn(file_path, perform_shuffle=False, repeat_count=1):\n",
    "    def decode_csv(line):\n",
    "        parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]])\n",
    "        label = parsed_line[-1:]  # Last element is the label\n",
    "        del parsed_line[-1]  # Delete last element\n",
    "        features = parsed_line  # Everything but last elements are the features\n",
    "        d = dict(zip(feature_names, features)), label\n",
    "        return d\n",
    "\n",
    "    dataset = (tf.data.TextLineDataset(file_path)  # Read text file\n",
    "               .skip(1)  # Skip header row\n",
    "               .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
    "    if perform_shuffle:\n",
    "        # Randomizes input using a window of 256 elements (read into memory)\n",
    "        dataset = dataset.shuffle(buffer_size=256)\n",
    "    dataset = dataset.repeat(repeat_count)  # Repeats dataset this # times\n",
    "    dataset = dataset.batch(32)  # Batch size to use\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고사항\n",
    "- TestLineDataset : DatasetAPI 는 파일 기반 데이터세트를 사용할 경우 많은 메모리 관련 작업을 자동으로 수행합니다.\n",
    "- shuffle : buffer_size 레코드를 읽은 후 순서를 셔플합니다. \n",
    "- map : 데이터 세트의 각 요소를 인수로 삼아 decode_csv 함수를 호출 합니다.\n",
    "- decode_csv: 각 줄을 필드로 분할하고 필요한 경우 기본값을 제공 합니다. 그런 다음, 필드 키 및 필드 값고 함께 dict를 반환합니다.\n",
    "  그러면 map 함수가 dict를 사용하여 데이터 세트의 각elem을 업데이트 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c6fbffcfed93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnext_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_input_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFILE_TRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Will return 32 random elements\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-56e60ec914e4>\u001b[0m in \u001b[0;36mmy_input_fn\u001b[1;34m(file_path, perform_shuffle, repeat_count)\u001b[0m\n\u001b[0;32m     10\u001b[0m     dataset = (tf.data.TextLineDataset(file_path)  # Read text file\n\u001b[0;32m     11\u001b[0m                \u001b[1;33m.\u001b[0m\u001b[0mskip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Skip header row\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                .map(decode_csv))  # Transform each elem by applying decode_csv fn\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mperform_shuffle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Randomizes input using a window of 256 elements (read into memory)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[0;32m    836\u001b[0m     \"\"\"\n\u001b[0;32m    837\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    839\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mParallelMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_map_func\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1826\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1828\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_as_variant_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\u001b[0m in \u001b[0;36madd_to_graph\u001b[1;34m(self, g)\u001b[0m\n\u001b[0;32m    486\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[1;34m\"\"\"Adds this function into the graph g.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_definition_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[1;31m# Adds this function into 'g'.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;34m\"\"\"Creates the function definition if it's not created yet.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m       \u001b[1;31m# Call func and gather the output tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemp_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m       \u001b[1;31m# There is no way of distinguishing between a function not returning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mtf_map_func\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   1812\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1813\u001b[0m       self._output_shapes = nest.pack_sequence_as(\n\u001b[1;32m-> 1814\u001b[1;33m           ret, [t.get_shape() for t in nest.flatten(ret)])\n\u001b[0m\u001b[0;32m   1815\u001b[0m       self._output_types = nest.pack_sequence_as(\n\u001b[0;32m   1816\u001b[0m           ret, [t.dtype for t in nest.flatten(ret)])\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1812\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1813\u001b[0m       self._output_shapes = nest.pack_sequence_as(\n\u001b[1;32m-> 1814\u001b[1;33m           ret, [t.get_shape() for t in nest.flatten(ret)])\n\u001b[0m\u001b[0;32m   1815\u001b[0m       self._output_types = nest.pack_sequence_as(\n\u001b[0;32m   1816\u001b[0m           ret, [t.dtype for t in nest.flatten(ret)])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'get_shape'"
     ]
    }
   ],
   "source": [
    "next_batch = my_input_fn(FILE_TRAIN, True)  # Will return 32 random elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator 소개\n",
    "Estimators는 이전에 TensorFlow 모델을 훈련시킬 때 작성해야 했던 사용구 코드를 상당히 줄여주는 상위 수준 API입니다.\n",
    "Estimators는 유연성이 뛰어나 모델에 특정한 요구사항이 있을 경우 기본 동작을 재정의 할 수 있습니다.\n",
    "\n",
    "### Estimator빌드 방법\n",
    "     * 미리만든 Estimators : 미리 정의된 Estimators로, 특정 유형의 모델을 생성하려고 만든 것입니다.\n",
    "     * Estimatro(기본클래스) : model_fn 함수를 사용하여 모델 생성 방법을 완벽히 제어할 수 있습니다. \n",
    "여기서는 미리만든 Estimators로서 DNNClassifier라는 것을 사용합니다.\n",
    "\n",
    "##  Estimators의 클래스 다이어그램\n",
    "\n",
    "![Estimators 클래스 다이어 그램](https://1.bp.blogspot.com/njTtnjOq_cE/Wbe772URrgI/AAAAAAAAD1Y/h1mWj6MGSzYg_KDuVXWBYeNqA4z5WRSpACLcBGAs/s1600/image2.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_master': '', '_evaluation_master': '', '_keep_checkpoint_max': 5, '_task_type': 'worker', '_model_dir': '/tmp/tf_dataset_and_estimator_apis', '_is_chief': True, '_train_distribute': None, '_global_id_in_cluster': 0, '_device_fn': None, '_save_summary_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_num_worker_replicas': 1, '_service': None, '_save_checkpoints_steps': None, '_num_ps_replicas': 0, '_log_step_count_steps': 100, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x10e197b70>, '_save_checkpoints_secs': 600, '_session_config': None, '_tf_random_seed': None}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the feature_columns, which specifies the input to our model\n",
    "# All our input features are numeric, so use numeric_column for each one\n",
    "feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]\n",
    "\n",
    "# Create a deep neural network regression classifier\n",
    "# Use the DNNClassifier pre-made estimator\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,  # The input features to our model\n",
    "    hidden_units=[10, 10],  # Two layers, each with 10 neurons\n",
    "    n_classes=3,\n",
    "    model_dir=PATH)  # Path to where checkpoints etc are stored\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 훈련\n",
    "\n",
    "다음과 같이  TensorFlow 코드 한 줄로 훈련을 수행할 수 있습니다.\n",
    "아래의 lamda:my_input_fn(FILE_TRAIN,True,8))은 Datasets과 Estimatorsㄹ를 연결한 부분 입니다. \n",
    "Estimators가 훈련, 평가 및 예측 작업을 수행하려면 데이터가 필요하며, input_fn을 사용하여 데이터를 가져옵니다.\n",
    "Estimators는 인수 없이 input_fn만 필요로 하므로, lamda를 사용하여 인수 없이 함수를 생성합니다.\n",
    "file_path, shuffle setting 및 repeat_count를 사용하여 input_fn을 호출합니다\n",
    "\n",
    "여기서는 my_inut_fn을 사용하여 이를 전달 합니다.\n",
    "FILE_TRAIN - 훈련 데이터 파일 \n",
    "True - Estimators에 데이터를 셔플하도록 지시합니다.\n",
    "8 - Estimators에 데이터 셍트를 8회 반복하도록 지시합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_dataset_and_estimator_apis/model.ckpt-30\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 30 into /tmp/tf_dataset_and_estimator_apis/model.ckpt.\n",
      "INFO:tensorflow:loss = 12.748289, step = 31\n",
      "INFO:tensorflow:Saving checkpoints for 60 into /tmp/tf_dataset_and_estimator_apis/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 11.037584.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x107951710>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train our model, use the previously function my_input_fn\n",
    "# Input to training is a file with training example\n",
    "# Stop training after 8 iterations of train data (epochs)\n",
    "classifier.train(\n",
    "    input_fn=lambda: my_input_fn(FILE_TRAIN, True, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련된 모델 평가\n",
    "\n",
    "모든 Estimator에 evaluate메서드가 포함되어있습니다.\n",
    "여기서는 약 93%의 정확도를 달성 했습니다. \n",
    "정확도를 높이는 방법은 프로그램을 단순히 계속 바복해서 실행하는 방법이 있습니다. \n",
    "또 다른 방법은 숨겨진 계층의 수나 각각의 숨겨진 계층에 포함된 노드 수를 조정하는 것입니다.\n",
    "자유롭게 이 방법을 실험해 보기를 권장합니다.\n",
    "단, model_dir=PATH에 지정된 디렉토리 제거에 유의하세요 DNNClassifier의 구조가 변경되기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-21-17:14:54\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_dataset_and_estimator_apis/model.ckpt-30\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-21-17:14:55\n",
      "INFO:tensorflow:Saving dict for global step 30: accuracy = 0.96666664, average_loss = 0.44668287, global_step = 30, loss = 13.400486\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 30: /tmp/tf_dataset_and_estimator_apis/model.ckpt-30\n",
      "Evaluation results\n",
      "   loss, was: 13.40048599243164\n",
      "   accuracy, was: 0.9666666388511658\n",
      "   global_step, was: 30\n",
      "   average_loss, was: 0.446682870388031\n",
      "Predictions on test file\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_dataset_and_estimator_apis/model.ckpt-30\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "Predictions on memory\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_dataset_and_estimator_apis/model.ckpt-30\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I think: [5.9, 3.0, 4.2, 1.5], is Iris Versicolor\n",
      "I think: [6.9, 3.1, 5.4, 2.1], is Iris Virginica\n",
      "I think: [5.1, 3.3, 1.7, 0.5], is Iris Sentosa\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate our model using the examples contained in FILE_TEST\n",
    "# Return value will contain evaluation_metrics such as: loss & average_loss\n",
    "evaluate_result = classifier.evaluate(\n",
    "    input_fn=lambda: my_input_fn(FILE_TEST, False, 4))\n",
    "print(\"Evaluation results\")\n",
    "for key in evaluate_result:\n",
    "    print(\"   {}, was: {}\".format(key, evaluate_result[key]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련된 모델을 이용한 예측\n",
    "이제 훈련된 모델이 있으므로, 평가 결과에 만족한다면 이를 사용하여 몇가지 입력 값을 기준으로 붓꽃을 예측할 수 있습니다.\n",
    "훈련 및 평가와 마찬가지로, 단일 함수 호출을 사용하여 예측합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test file\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_dataset_and_estimator_apis/model.ckpt-60\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Predict the type of some Iris flowers.\n",
    "# Let's predict the examples in FILE_TEST, repeat only once.\n",
    "predict_results = classifier.predict(\n",
    "    input_fn=lambda: my_input_fn(FILE_TEST, False, 1))\n",
    "print(\"Predictions on test file\")\n",
    "for prediction in predict_results:\n",
    "    # Will print the predicted class, i.e: 0, 1, or 2 if the prediction\n",
    "    # is Iris Sentosa, Vericolor, Virginica, respectively.\n",
    "    print(prediction[\"class_ids\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 메모리 내 데이터를 기반으로 하는 예측\n",
    "위에 나와 있는 코드에서는 파일에 저장된 데이터를 기반으로 예측을 수행하도록 FILE_TEST를 지정했지만\n",
    "메모리를 기반으로 예측하는 방법도 있습니다.\n",
    "다음과 같이 메모리 구조를 사용하도록 DatasetAPI를 구성하면 됩니다.\n",
    "Dataset.from_tensor_slides()는 메모리에 적합한 작은 데이터세트용으로 설계되었습니다. \n",
    "훈련 및 평가를 위해 TextLineDataset을 사용하는 경우 메모리가 셔플 버퍼와 배치 크기를 감당할 수 있는 한 원하는 만큼 큰 파일을 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on memory\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_dataset_and_estimator_apis/model.ckpt-60\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I think: [5.9, 3.0, 4.2, 1.5], is Iris Versicolor\n",
      "I think: [6.9, 3.1, 5.4, 2.1], is Iris Virginica\n",
      "I think: [5.1, 3.3, 1.7, 0.5], is Iris Sentosa\n"
     ]
    }
   ],
   "source": [
    " #Let create a dataset for prediction\n",
    "# We've taken the first 3 examples in FILE_TEST\n",
    "prediction_input = [[5.9, 3.0, 4.2, 1.5],  # -> 1, Iris Versicolor\n",
    "                    [6.9, 3.1, 5.4, 2.1],  # -> 2, Iris Virginica\n",
    "                    [5.1, 3.3, 1.7, 0.5]]  # -> 0, Iris Sentosa\n",
    "\n",
    "\n",
    "def new_input_fn():\n",
    "    def decode(x):\n",
    "        x = tf.split(x, 4)  # Need to split into our 4 features\n",
    "        return dict(zip(feature_names, x))  # To build a dict of them\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(prediction_input)\n",
    "    dataset = dataset.map(decode)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_feature_batch = iterator.get_next()\n",
    "    return next_feature_batch, None  # In prediction, we have no labels\n",
    "\n",
    "# Predict all our prediction_input\n",
    "predict_results = classifier.predict(input_fn=new_input_fn)\n",
    "\n",
    "# Print results\n",
    "print(\"Predictions on memory\")\n",
    "for idx, prediction in enumerate(predict_results):\n",
    "    type = prediction[\"class_ids\"][0]  # Get the predicted class (index)\n",
    "    if type == 0:\n",
    "        print(\"I think: {}, is Iris Sentosa\".format(prediction_input[idx]))\n",
    "    elif type == 1:\n",
    "        print(\"I think: {}, is Iris Versicolor\".format(prediction_input[idx]))\n",
    "    else:\n",
    "        print(\"I think: {}, is Iris Virginica\".format(prediction_input[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard 실행\n",
    "아래 코드를 command 창에서 실행 후 \n",
    "localhost:6006 접속 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensorboard' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3be17a5ec9eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Replace PATH with the actual path passed as model_dir argument when the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# DNNRegressor estimator was created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtensorboard\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tensorboard' is not defined"
     ]
    }
   ],
   "source": [
    "# Replace PATH with the actual path passed as model_dir argument when the\n",
    "# DNNRegressor estimator was created.\n",
    "tensorboard --logdir=/tmp/tf_dataset_and_estimator_apis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
